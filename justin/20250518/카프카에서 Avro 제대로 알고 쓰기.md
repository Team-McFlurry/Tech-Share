# 카프카에서 Avro 제대로 알고 쓰기
# 1. 직렬화(Serialization)

Avro를 제대로 이해하려면 먼저 **직렬화(Serialization)와 역직렬화(Deserialization)**의 개념과 목적을 알아야 한다.

> “객체나 데이터 구조를 네트워크나 저장소(예: 배열 버퍼 또는 파일 형식)를 통한 전송에 적합한 형식으로 변환하는 프로세스입니다.”
> 
> – MDN Web Docs

여기서 핵심은 **전송에 적합한 형식으로 변환한다는 것**이다.

컴퓨터에게 있어 전송에 적합한 형식이란 결국 **바이트 배열**이다. (실제로 전송할 땐 이 바이트 배열을 다시 이진 데이터로 인코딩하는 과정이 추가된다.)

즉, 객체를 바이트 배열로 변환하는 과정을 **직렬화**, 그 반대로 바이트 배열을 다시 객체로 변환하는 과정을 **역직렬화**라고 한다.

이러한 직렬화/역직렬화 덕분에 서로 다른 시스템은 객체 데이터를 **네트워크나 파일**을 통해 주고받을 수 있다.

그러나 Java의 native serialization은 자바 애플리케이션끼리 통신할 때만 사용할 수 있는데, 이는 **언어 종속적**이기 때문이다.

그래서 등장한 것이 **언어에 독립적인 공통 직렬화 시스템이고,** 대표적인 예가 바로 **JSON**이다.

다음 절에서는 이 JSON을 기준으로, 이번 글의 주제인 **Avro**에 대해 본격적으로 다뤄볼 것이다.



# 2. JSON vs Avro

## JSON

**JSON**(JavaScript Object Notation)은 원래 자바스크립트에서 브라우저 간 데이터 교환을 위해 고안된 **텍스트 기반 데이터 포맷**이다. 

현재는 언어에 독립적인 형식으로 자리 잡아, 거의 모든 시스템에서 **데이터 통신의 표준 포맷**처럼 사용되고 있다. 

JSON은 사람이 읽고 쓰기 쉬우며, 대부분의 언어에서 이를 지원하는 **라이브러리와 도구들이 잘 갖춰져 있다.**

그러나 **데이터 엔지니어링 관점에서 JSON은 몇 가지 근본적인 한계를 갖는다.**

1. **필드 이름이 항상 데이터와 함께 포함**되므로, 불필요한 용량 낭비가 발생한다.
2. 텍스트를 기반으로 하다 보니, 바이너리 포맷에 비해 직렬화/역직렬화 성능이 떨어진다.
3. **스키마가 명확히 정의되어 있지 않기 때문에**, 데이터의 일관성을 유지하기 어렵다.

이러한 제약들이 **크게 문제되지 않는 환경**에서는 JSON이 여전히 매우 유용한 선택이다.

(데이터 규모가 작고, 고성능이 크게 요구되지 않으며, 데이터 구조가 자주 바뀌지 않는 경우)

하지만 실시간으로 대용량 데이터가 유입되고, 안정성**과 성능이 중요한 데이터 파이프라인**이라면 더 나은 대안이 필요하다.

이런 상황에서 대표적으로 사용되는 것이 바로 **Avro**다.

---

## Apache Avro

**Apache Avro**는 원래 **하둡(Hadoop)** 프로젝트의 일부로 개발된 **데이터 직렬화 시스템**이다. 현재는 Kafka 기반의 이벤트 스트리밍 환경에서 **대표적인 이진 직렬화 포맷**으로 자리 잡고 있다.

Avro는 다음과 같은 특징을 가진다

- **스키마 기반 직렬화**
    - Avro는 JSON 포맷의 **스키마(schema)** 를 사용해 데이터를 정의하고 직렬화한다.
    - 이를 통해 **타입 안정성**과 **데이터 구조의 일관성**을 보장할 수 있다.


- **공간 효율성**
    - JSON과 달리, 필드 이름을 데이터에 포함시키지 않는다.
    - 스키마로 구조를 따로 관리하고, **값만 순서대로 저장**하기 때문에 중복 오버헤드가 없고 크기도 작다.
    - Kafka 환경에서는 **Schema Registry**에 스키마를 등록하고, 데이터에는 **스키마 ID만 포함**시켜 전송함으로써 공간 효율성을 더욱 극대화할 수 있다.

    
- **성능**
    - Avro는 **바이너리 포맷**이기 때문에, 텍스트 기반인 JSON보다 **직렬화와 역직렬화 속도가 빠르다.**
    - 이 덕분에 실시간 스트리밍에 적합하다.
    

물론 Avro를 도입하면 스키마를 관리해야 하는 부담이 생긴다.

하지만 현실적인 데이터 파이프라인에서는 **스키마 진화(schema evolution)** 가 불가피하고, 이 과정을 명시적으로 설계할 수 있다는 점에서 Avro는 시스템의 안정성과 유지보수성을 오히려 높여주는 역할을 한다.

---
**JSON과 Avro는 엄밀히 말하면 서로 다른 범주에 속한다.**

JSON은 단순한 **데이터 포맷**이고, Avro는 **직렬화 시스템 전체**를 의미한다.

이 관점에서 볼 때, 흔히 사용하는 “_JSON으로 직렬화한다_"는 표현은 기술적으로는 정확하지 않다.

실제로는 **Jackson**, **Gson** 같은 라이브러리를 통해 객체를 **JSON 문자열로 변환한 다음**, 다시 **UTF-8 등의 문자 인코딩 방식으로 바이트 배열로 변환**하는 과정이 필요하다.

이 모든 과정이 라이브러리 내부에서 한 번에 처리되기 때문에, 실무에서는 편의상 "JSON 직렬화"라는 표현을 관용적으로 사용하곤 한다.

하지만 기술적으로는, JSON은 **데이터 포맷을 말하며**, Avro처럼 **직렬화 시스템 자체를 정의하는 범위까지 포괄하지는 않는다.**

---
# 3. Avro 구조

## Avro Schema 및 객체 (Java)

아래는 간단한 **Avro 스키마**의 예시다. `User`라는 레코드 타입에 세 개의 필드가 정의되어 있다.

```json
{
  "namespace": "example.avro",
  "type": "record",
  "name": "User",
  "fields": [
    {"name": "name", "type": "string"},
    {"name": "favorite_number", "type": ["null", "int"]},
    {"name": "favorite_color", "type": ["null", "string"]}
  ]
}
```

Java 프로젝트에서 Avro 라이브러리를 활용하면, 이 스키마 파일을 기반으로 객체 클래스를 자동 생성할 수 있다.
![Pasted image 20250518063940.png](resource%2FPasted%20image%2020250518063940.png)
생성된 클래스는 Avro의 `SpecificRecord` 인터페이스를 구현하며, 타입 안정성을 제공한다.

Avro 데이터를 Java에서 다룰 때는 주로 다음 두 가지 방식 중 하나로 역직렬화를 수행한다

- `SpecificRecord` (정적으로 생성된 타입 클래스)
- `GenericRecord` (동적 타입 구조)

이 두 방식의 차이점은 뒤에서 다시 살펴본다.

---

## Avro Message

**Avro 데이터를 저장하거나 전송하는 방식**은 사용하는 컨텍스트에 따라 크게 두 가지로 나뉜다

### 1. Object Container File (파일 컨테이너)

- **스키마가 헤더에 포함됨**
- 단일 파일에 스키마 + 데이터가 함께 들어 있기 때문에 **자급자족(self-contained)** 형태다
- 어떤 시스템이든 파일만 받으면 스키마 없이도 데이터를 읽을 수 있다

### 2. Kafka 메시지 스트림
![Pasted image 20250518063911.png](resource%2FPasted%20image%2020250518063911.png)

Kafka에서는 **스키마를 메시지에 직접 포함하지 않는다**.

**스키마는 외부 Schema Registry에 저장하고,** 메시지에는 스키마 ID(4바이트)만 포함된다.

컨슈머는 메시지를 읽을 때 이 ID를 기반으로 Schema Registry에서 스키마를 조회한 뒤 역직렬화를 수행한다.

이 방식은 메시지 페이로드를 작게 유지해 **전송 효율성을 극대화**할 수 있다.

# 4. Kafka 환경에서 Avro 사용

## Schema Registry

Schema Registry는 Kafka 프로듀서와 컨슈머 사이에서 Avro 스키마를 중앙에서 관리하는 구성 요소다.

아래 그림처럼, 프로듀서와 컨슈머는 Schema Registry와 연동하여 스키마 ID를 주고받는 방식으로 Avro 데이터를 교환한다.

![Pasted image 20250518063848.png](resource%2FPasted%20image%2020250518063848.png)

## Schema Registry의 **주요 기능**

- **중앙 스키마 저장소**
    - 모든 Avro 스키마를 중앙에서 관리함으로써, 데이터 일관되게 유지할 수 있다.
    - 서로 다른 서비스 간에도 동일한 subject의 스키마를 참조함으로써 스키마 불일치 오류를 줄일 수 있다.


- **스키마 버전 관리**
    - 하나의 subject 아래 여러 버전의 스키마를 저장하고, 각 버전에는 고유한 **스키마 ID**와 **버전 번호**가 부여된다.
    - 이를 통해 과거 스키마와 신규 스키마를 동시에 보존하며, 데이터 호환성을 유연하게 관리할 수 있다.


- **스키마 호환성 검증**
    - 새로 등록되는 스키마는 기존 버전과의 호환성 규칙(compatibility mode)을 자동으로 검사받는다.
    - 대표적인 모드는 `BACKWARD`, `FORWARD`, `FULL` 등이 있으며, **Confluent의 기본 설정은 BACKWARD**다.
    - 예를 들어 BACKWARD 모드에서는 새 스키마가 이전 버전의 데이터를 읽을 수 있어야 하며, 호환되지 않는 변경(예: 필드 타입 변경, 필수 필드 추가 등)이 감지되면 등록이 거부된다.
    - 이 과정을 통해 스키마 진화로 인한 데이터 파손이나 역직렬화 오류를 사전에 방지할 수 있다.


---

## Schema Registry를 활용한 Kafka 데이터 흐름

### 프로듀서 측

- Kafka 프로듀서는 `KafkaAvroSerializer`를 설정해 Avro 데이터를 직렬화한다.
- 이때 해당 Avro 스키마는 자동으로 Schema Registry에 등록되며, 이미 등록된 스키마와 동일한 경우에는 생략된다.
- 기본적으로 스키마 subject 이름은 `{토픽명}-key`, `{토픽명}-value` 형식으로 설정된다.
- 새 스키마가 등록될 경우, Schema Registry는 기존 스키마와의 호환성을 검사하고, 조건에 맞지 않으면 HTTP 오류를 반환하며, 이 과정은 Serializer 내부에서 예외(SerializationException)로 처리된다.
- 등록이 성공하면 Registry가 발급한 고유한 스키마 ID(4 bytes)가 Avro 메시지의 맨 앞에 포함되고, 이 바이트 배열이 Kafka로 전송된다.

### 컨슈머 측

- Kafka 컨슈머는 `KafkaAvroDeserializer`를 사용해 데이터를 역직렬화한다.
- 수신한 메시지에서 스키마 ID를 먼저 읽고, 이 ID를 통해 Schema Registry에 등록된 스키마를 조회한다.
- 조회는 Schema Registry의 REST API를 통해 자동으로 수행되며, 이때 가져온 스키마를 기반으로 메시지 바이트를 해석한다.
- 이후, 설정에 따라 `GenericRecord` 또는 `SpecificRecord`로 Avro 객체를 복원하고, 이를 통해 애플리케이션이 이해할 수 있는 구조화된 데이터로 사용할 수 있다.

---

## GenericRecord vs SpecificRecord

Avro를 Kafka와 함께 사용할 때는 **GenericRecord**와 **SpecificRecord** 두 가지 방식으로 데이터를 다룰 수 있다

```kotlin
// GenericRecord 예제
ConsumerRecord<String, GenericRecord> records = consumer.poll(Duration.ofMillis(100));
for (record in records) {
    val name = record.get("name").toString()
}

// SpecificRecord 예제
ConsumerRecord<String, User> records = consumer.poll(Duration.ofMillis(100));
for (record in records) {
    val name = record.name
}
```

### SpecificRecord

- 스키마로부터 생성된 클래스(POJO)를 사용하며, `SpecificRecord` 인터페이스를 구현한다.
- **컴파일 타임에 데이터 구조가 확정되므로 타입 안전성**이 높고, 코드가 간결하다.
- 스키마 변경 시 **코드 재생성과 애플리케이션 재배포**가 필요하다.
- **스키마가 자주 바뀌지 않고 구조가 명확한 경우**에 적합하다.


### GenericRecord

- 스키마를 런타임에 로드하고, 일반화된 Avro 객체(`GenericRecord`)로 데이터를 다루는 방식이다.
- 특정 클래스 없이도 다양한 스키마의 데이터를 처리할 수 있어, **스키마 진화나 다중 이벤트 타입**에 유리하다.
- 필드 접근 시 문자열 기반이므로 **타입 안정성이 낮고**, 코드 실수에 취약하다.
- **Schema Registry와 통합**해 스키마를 동적으로 가져올 수 있다.

---

# 5. Avro 스키마 호환성과 진화 (Schema Evolution)

## 스키마 호환성

스키마 레지스트리의 핵심 기능 중 하나는 바로 스키마 호환성 관리다.

운영 중인 서비스에서는 데이터 구조가 시간이 지나면서 자연스럽게 변화하게 된다. 하지만 Avro처럼 스키마 기반 직렬화 시스템에서는 이러한 변화가 곧바로 시스템 장애나 데이터 역직렬화 오류로 이어질 수 있다.

따라서 스키마 변경이 기존 시스템과 충돌하지 않도록 호환성 정책(Compatibility Policy)을 적용해주는 것이 매우 중요하다.
![Pasted image 20250518063806.png](resource%2FPasted%20image%2020250518063806.png)

예를 들어 위 그림은 v1 스키마를 사용하는 두 프로듀서가 Kafka 토픽에 데이터를 적재하고, 컨슈머는 같은 v1 스키마로 메시지를 소비하는 상황을 나타낸다.

그런데 이 중 한 프로듀서가 스키마를 v2로 변경하게 되면, 컨슈머는 여전히 v1 스키마를 기준으로 메시지를 읽기 때문에, 스키마가 호환되지 않는 경우 역직렬화 오류가 발생할 수 있다.

만약 v2와 v1 스키마 간의 호환성이 유지된다면, 컨슈머는 기존의 v1 스키마로도 v2로 작성된 데이터를 문제없이 읽을 수 있다.

즉, 스키마 간 호환성이 확보된 경우에는 서로 다른 버전의 스키마를 사용하는 프로듀서와 컨슈머가 함께 동작할 수 있다.

![Pasted image 20250518063754.png](resource%2FPasted%20image%2020250518063754.png)

위 그림은 Avro 스키마 진화의 예시를 보여준다.

기존 `v1` 스키마에는 `name`과 `age(기본값: 20)` 두 개의 필드가 정의되어 있으며, 오른쪽에는 이 스키마가 두 가지 방식으로 `v2` 버전으로 변경되는 사례가 나와 있다.

1. `age` 필드를 **삭제**한 경우
2. `country`라는 **새로운 필드를 추가**한 경우

이때 컨슈머가 여전히 `v1` 스키마를 사용하고 있다고 가정하면, 두 가지 `v2` 버전의 데이터를 모두 **정상적으로 역직렬화할 수 있다**.

- 첫 번째 경우는 `v2` 데이터에 `age` 필드가 없지만, `v1`에서는 기본값(20)을 사용할 수 있으므로 문제 없음
- 두 번째 경우는 `v2`에 새로 추가된 `country` 필드가 있지만, `v1` 스키마에는 해당 필드가 없기 때문에 무시된다

이처럼 **낮은 버전의 스키마(`v1`)로 높은 버전(`v2`)의 데이터를 읽을 수 있는 경우**, 이를 **Forward 호환성이 만족된다고** 한다.

<br>

그렇다면 반대의 경우, 즉 **`v2` 스키마로 `v1` 데이터를 읽을 수 있는지**를 생각해보자.

이 경우는 **Backward 호환성**이 성립해야 한다.

`age` 필드가 삭제된 버전은 Backward 호환성에 문제가 없다. `v2`에서 해당 필드를 제거했으므로 `v1` 데이터를 읽을 때 무시할 수 있기 때문이다.

그러나 `country` 필드가 추가된 버전은 **Backward 호환성을 만족하지 않는다.**

왜냐하면 `v2` 스키마는 `country` 필드를 필수로 요구하지만, `v1` 데이터에는 해당 필드가 없고, 기본값도 정의되어 있지 않기 때문에 역직렬화 시 오류가 발생한다.

즉, **새로운 필드를 추가할 경우에는 반드시 기본값을 정의해야 Backward 호환성을 유지할 수 있다.**

---

## SpecificRecord와 호환성 정책

이쯤에서 이런 의문이 생길 수 있다

> “Schema Registry에서 해당 메시지의 스키마를 가져와서 읽으면 되는 거 아닌가?”

**`GenericRecord`** 의 경우에는 맞는 말이다. 런타임에 스키마를 동적으로 변경할 수 있기 때문이다.

하지만 **`SpecificRecord`를 사용할 경우엔 그렇지 않다.** `SpecificRecord`는 컴파일 시점에 스키마가 고정되며, **런타임에 변경할 수 없다.**

그럼에도 불구하고, 역직렬화를 위해서는 **메시지의 writer 스키마**를 Schema Registry에서 조회해야 한다.

이때 Avro는 **Writer Schema (from Schema Registry)** 와 **Reader Schema (코드에 컴파일된 POJO 클래스의 스키마)** 를 비교하여 호환성을 검증하고, 역직렬화를 시도한다.

즉, `SpecificRecord`를 사용하는 경우에는 **Reader와 Writer 스키마가 다를 수 있고**, 이 상황에서 Avro의 **스키마 호환성 정책에 따라 역직렬화 가능 여부가 결정**되는 것이다.

스키마 레지스트리에서는 다음과 같은 **호환성 정책(Compatibility Policy)**을 설정할 수 있으며, 새 스키마가 등록될 때 기존 스키마들과 비교하여 **지정된 정책을 만족하는지 검증한다.**

- **Backward(후방 호환)** : 신규 스키마로 기존 데이터 읽기 가능 (예: 필드 삭제, 기본값이 있는 필드 추가)
- **Forward(**전방 **호환)** : 기존 스키마로 신규 데이터 읽기 가능 (예: 필드 추가, 기본값 없는 필드 삭제)
- **Full(완전 호환)**: **Backward + Forward 모두 만족**

## 호환성에 따른 배포 순서 전략

새로운 스키마로 애플리케이션을 배포해야 할 때.

예를 들어 새 스키마가 **Backward 호환은 만족하지만 Forward 호환은 만족하지 않는 경우**, **배포 순서**가 매우 중요해진다.

아래의 예시 데이터 파이프라인에서 프로듀서와 컨슈머는 모두 `SpecificRecord`를 사용하고 있으며, 이 경우 **컨슈머를 먼저 배포해야 한다.**
![Pasted image 20250518063634.png](resource%2FPasted%20image%2020250518063634.png)

다음은 **컨슈머와 프로듀서의 배포 순서에 따른 결과 차이**를 정리한 것이다.

- 컨슈머를 먼저 배포하는 경우
    - Backward 호환성을 만족하므로 컨슈머(v2)가 프로듀서(v1)의 데이터를 읽을 수 있음 → 문제 없음
- 프로듀서를 먼저 배포하는 경우
    - Forward 호환성을 만족하지 않으므로 컨슈머(v1)가 새로운 데이터(v2)를 읽을 수 없음 → 문제 발생

따라서 이 경우에는 컨슈머를 먼저 배포해야 문제가 발생하지 않는다.

잘못된 배포 순서는 Kafka에 정상 전송된 메시지가 **역직렬화 오류로 소비되지 못하는 상황**을 유발할 수 있다.