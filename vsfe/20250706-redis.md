# 신나는 Redis 장애 대응

회사를 다닌지 그렇게 길지도 않았지만, Redis로 장애를 세번이나 뚜드려 맞았습니다.

레거시 구조에 의해 맞은 것도 있었고 휴먼 에러도 있었지만, 원인 분석을 열심히 하다보니 여러모로 도움은... 되었네요.

Redis 장애의 문제점은, 보통 **뭐가 문제인지 바로 안보인다**는 점인데, 단순 자원 사용량 모니터링으로 모든 문제를 파악할 수 없다는게 제일 큰 것 같습니다.
특히, 클러스터 환경을 구축하고 있는 상황에서도 장애가 터졌다는 것은 단순 Redis의 문제가 아닌 뭔가가 종합적으로 꼬여있는 경우가 더 많기도 하고요.

이번에는, 제가 뚜드려 맞았던 장애 중 두 가지를 가볍게 살펴보도록 하겠습니다. (하나는 C 서버 클라이언트 관련 내용이라 생략합니다 ㅎㅎ)

## EASY - Redis Cluster 구축 장애
신규 클러스터가 추가 되어서, 구축하는 과정에서 발생했던 장애 입니다.
새로운 리전이 추가 되어서 클러스터 작업을 하다, **기존 클러스터의 장비를 잘못 추가하는 바람에,** 기존 클러스터가 터져버리고 (...) 데이터가 없는 상태의 클러스터가 새로 구축되어 버렸습니다.
머... 일부 데이터 유실은 어쩔 수 없지 하고 넘어가고 싶었지만, 이번엔 Lettuce가 문제였습니다.

Lettuce는 다른 클라이언트와 달리, (같이 운영하고 있던 C 서버에서는 문제 없었음) **클러스터 재 구성 시 이를 반영하지 못하기에,** 서버를 재구동 하거나 클러스터 구성을 완벽하게 원복해야 했습니다.
즉, 현재 상황에서는 구성이 바뀌었기 때문에, Redis를 참조하는 모든 로직에 문제가 생길 수 밖에 없었죠.

잽싸게 문제를 인지하고 이를 복구했다면 금방 끝낼 수 있는 문제였지만, 당시 상황이 상황인지라... 이를 대응하던 인력이 Redis에 대한 지식이 많지 않았습니다.
특히나 Redis의 클러스터를 직접 관리하는 상황에선, **모든 작업은 양쪽 장비에 모두 수행해야 한다는 점**을 인지해야 하는데, 그러지 못해 복구 시간이 좀 더 길어졌습니다.

(예를 들어, `forgot` 명령어는 추가된 장비에도, 기존 장비에도 모두 수행해야 연결을 아예 끊어버릴 수 있는데, 이를 한쪽에만 수행하는 바람에 오히려 연결만 끊기고 장애 복구는 안 되는 상황이 지속되었습니다.)
결국 문제 상황을 인지하고 일단 기존 장비가 master가 되도록 failover를 수행했고, (원래 장비가 다시 master가 되다보니 클러스터의 정보가 복구된 걸로 Lettuce가 판단되어 서비스 자체는 복구되었습니다.) 양쪽에 forgot을 수행하는 것으로 최종 작업을 완료했습니다.

물론... 데이터가 다 날아가서 눈물이 조금 나더라고요.


## HARD - Redis Cluster Full Sync 장애
진짜 갑자기 Redis가 터져버린 이슈입니다 (으악)

어느날 이유 없이 사이트 접속이 안 되는 상황이 발생했습니다. 한가지 요상한 점이라면 서버 API는 잘 동작했지만 사이트 접속은 안 되었던 거였습니다.
사실, 이는 페이지를 띄울 때 사용하는 특정 API가 레디스에 강하게 의존적이어서 발생한 문제였습니다. 결국 어떤 API에서 장애가 발생한 것인지 쉽게 파악할 수 있었고, 자연스럽게 Redis 장애로 의심하게 되었습니다.

근데 문제는 redis의 자원 사용량이 "CPU만" 높았다는 것 입니다.

자원 모니터링 및 알람을 확인해보니 redis의 CPU 사용률이 100%였고, 조금 더 깊게 파악하다보니 Redis Cluster의 3대의 Master가 동시에 Slave와 Full Sync를 수행하고 있음을 확인할 수 있었습니다.
말은 저렇게 했지만, 실제로는 이를 파악하는데 상당한 시간이 걸렸습니다.

결국 모든 Master는 Full Sync를 지속적으로 수행하다보니 성능이 급격하게 저하되었고, 자연스럽게 애플리케이션에도 영향을 미쳐 장애가 발생한 것 이었습니다.
주담당자였던 분이 Master를 Restart 했으나, 여전히 Full Sync가 지속적으로 발생했고, 약간의 고민이 진행된 이후 Failover를 수행한 끝에 이 문제는 해결되었습니다.

이슈 해결은 위 EASY 파트랑 별 차이 없어보이죠?

보통 이런 장애가 발생하면 장애보고서를 작성해야 하기에, 보고서를 작성하며 장애의 원인을 파악해야 합니다.
가장 먼저, 많은 분들이 작성한 애플리케이션 코드의 로직을 의심했지만, 장애 발생 현상을 생각해보면 모든 Master가 Slave에게 Full Sync를 수행하는 것의 원인이 애플리케이션이 되긴 어렵다고 판단했고, 인프라 담당자를 통해 문의해보니 네트워크 순단으로 인한 물리적 원인으로 보인다는 결론을 내놓았습니다. 그렇다면 왜 Full Sync가 지속적으로 반복되는지를 판단해야 했는데, 관련된 이슈와 연관된 글고 거의 없었고, Java Client나 Redis 자체에도 issue로 올라온 것이 없다보니, 결국 Redis의 Cluster 구성 동작 구조와 Redis Property를 하나하나 분석해보며 원인을 파악하고자 했습니다.

조사 결과, 발생 원인은 다음과 같았습니다.

- 네트워크 순단으로 인한 Slave 연결 지연, 이로 인해 Sync가 밀리게 되고, Full Sync로 전환
- Full Sync 과정에서 Master는 자신의 데이터를 덤프화 시켜야 하고, 이 과정에서 Replication Buffer가 비워지지 않고 지속적으로 쌓이게 됨
- Redis 설정에 포함된 **client-output-buffer-limit** 옵션이 기본값으로 설정되어 있기 때문에, **psync를 통한 버퍼링 과정에서 64MB 버퍼가 60초 이상 남아있다보니 Slave가 Close** 되어버림
- 그러나 Slave는 아무 데이터도 갖고 있지 않기 때문에, 다시 Full Sync 요청을 수행함
- 무한 반복

이 구조를 이해하고 나니, 장애 대응 시점의 문제 해결이 안 되었던 이유, (Master Restart - Slave의 연결 정보가 남아있는 이상,
Full Sync 발생할 수 밖에 없음 + Redis Persistence 때문에 Master는 데이터가 채워져 있음) 그리고 결국 문제가 해결된 이유 (Master -> Slave Failover - Failover 절차에서는 Client 업데이트가 수행되지 않으므로, buffer가 채워지지 않음) 도 설명이 되었습니다.
후속 대응으로 buffer-limit을 256M 로 조정하고, (256M 선정의 기준은 Redis에서 권장하는 전체 Memory 8G의 10% 이내 수준으로 정했습니다.)  client의 timeout 도 조정하는 등 추가 작업을 수행했습니다.

물론... 결국 크고 작은 장애들을 뚜드려 맞다보니 자체 관리 Redis에서 전사 차원에서 관리하는 공용 Redis 풀로 이전을 하게 되었지만, 이 과정에서 배운 컴포넌트의 내부 구조와 레플리카 원리를 많이 파고들었던 것 같습니다.